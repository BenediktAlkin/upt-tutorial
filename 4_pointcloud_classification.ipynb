{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup environment\n",
    "!pip install kappamodules\n",
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# torch_cluster often doesnt support newest pytorch version which is by default installed in colab -> install older one\n",
    "!pip install torch==2.3.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# install torch_scatter\n",
    "!pip install torch_scatter torch_cluster -f https://data.pyg.org/whl/torch-2.3.0+cu121.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# check successful setup\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.get_device_name(device)\n",
    "from torch_geometric.nn.pool import radius_graph\n",
    "from torch_scatter import segment_csr"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# clone repo\n",
    "!git clone https://github.com/BenediktAlkin/upt-minimal.git\n",
    "%cd upt-minimal"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4 Pointcloud Classification\n",
    "\n",
    "Now we know how the model and the decoder works, but we've only trained with images (regular grid data).\n",
    "We'll want to train UPT on irregular grid data, so lets do it. First, we need a dataset but since we\n",
    "are already familiar with CIFAR10, we'll simply treat it as a pointcloud. So every pixel is a point in 2D\n",
    "space that has an RGB value. So essentially, we convert CIFAR10 into a sparse tensor representation\n",
    "which we'll call `SparseCIFAR10`.\n",
    "\n",
    "To illustrate the sparse representation better, we'll simply drop some pixels in the input (which we\n",
    "can now easily do because the encoder with a supernode pooling as initial layer can handle arbitrary\n",
    "pointclouds).\n",
    "\n",
    "<img width=\"60%\" src=\"https://raw.githubusercontent.com/BenediktAlkin/upt-minimal/main/schematics/upt_sparse_classifier.svg\">\n",
    "\n",
    "Now dataloading gets a bit trickier, because we need supernodes and we need to create a sparse tensor \n",
    "via the `collate_fn` of the pytorch [DataLoader](https://pytorch.org/docs/stable/data.html).\n",
    "We'll do this via an object, which we'll call `SparseImageClassifierCollator`.\n",
    "\n",
    "To start, lets initialize the dataset and visualize a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# initialize CIFAR10\n",
    "from upt.datasets.sparse_cifar10_classifier_dataset import SparseCIFAR10ClassifierDataset\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_dataset = SparseCIFAR10ClassifierDataset(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    # use half of the inputs for training (32x32 pixels = 1024)\n",
    "    num_inputs=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now plotting is a bit trickier because we have a sparse tensor\n",
    "# we'll make a scatter plot where each point is colored with the \n",
    "# RGB value of the pixel\n",
    "import matplotlib.pyplot as plt\n",
    "import einops\n",
    "\n",
    "# get dense sample\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "train_dataset_dense = CIFAR10(root=\"./data\", train=True, download=True)\n",
    "dense_sample = train_dataset_dense[0][0]\n",
    "dense_sample = ToTensor()(dense_sample)\n",
    "# get sparse sample\n",
    "sample = train_dataset[0]\n",
    "x, y = sample[\"input_pos\"].unbind(1)\n",
    "c = sample[\"input_feat\"]\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(4, 2))\n",
    "axes[0].imshow(einops.rearrange(dense_sample, \"three height width -> height width three\"))\n",
    "axes[0].set_title(\"dense input\")\n",
    "axes[0].set_axis_off()\n",
    "axes[1].scatter(y, 31 - x, c=c, marker=\"s\", s=10)\n",
    "axes[1].set_title(\"sparse input\")\n",
    "axes[1].set_axis_off()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lets see how a forward pass looks like for a single sample (so we don't need a collator yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_pointcloud.shape: torch.Size([1, 64, 192])\n"
     ]
    }
   ],
   "source": [
    "from upt.models.encoder_supernodes import EncoderSupernodes\n",
    "\n",
    "encoder = EncoderSupernodes(\n",
    "    # CIFAR has 3 channels (RGB)\n",
    "    input_dim=3,\n",
    "    # CIFAR is an image dataset -> 2D\n",
    "    ndim=2,\n",
    "    # there are 32x32 pixels so positions are in [0, 31], to have roughly the same input as a ViT\n",
    "    # with patch_size=4, we'll use radius slighly larger than 2\n",
    "    radius=3,\n",
    "    # if we split a 32x32 image into 8x8 gridpoints, each point would cover 4x4 pixels, i.e. 16 pixels (=nodes)\n",
    "    # since we sample supernodes randomly and use a larger radius, it can happen that more than 16 nodes\n",
    "    # are in the radius of a supernode, so we'll use at maximum 32 connections to each supernode\n",
    "    max_degree=32,\n",
    "    # dimension for the supernode pooling -> use same as ViT-T latent dim\n",
    "    gnn_dim=192,\n",
    "    # ViT-T latent dimension\n",
    "    enc_dim=192,\n",
    "    enc_num_heads=3,\n",
    "    # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "    enc_depth=4,\n",
    "    # the perceiver is optional, it changes the size of the latent space to NUM_LATENT_TOKENS tokens\n",
    "    # perc_dim=dim,\n",
    "    # perc_num_heads=num_heads,\n",
    "    # num_latent_tokens=32,\n",
    ")\n",
    "\n",
    "# for now, we only encode 1 sample, so we need to generate some supernode indices\n",
    "# later the collator will take care of this\n",
    "sample = train_dataset[0]\n",
    "# select 64 random pixels as supernodes\n",
    "supernode_idxs = torch.randperm(len(sample[\"input_feat\"]))[:64]\n",
    "# and we need a batch_idx tensor\n",
    "batch_idx = torch.zeros(len(sample[\"input_feat\"]), dtype=torch.long)\n",
    "encoded_pointcloud = encoder(\n",
    "    input_feat=sample[\"input_feat\"],\n",
    "    input_pos=sample[\"input_pos\"],\n",
    "    supernode_idxs=supernode_idxs,\n",
    "    batch_idx=batch_idx,\n",
    ")\n",
    "print(f\"encoded_pointcloud.shape: {encoded_pointcloud.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this setting, we dont really need an approximator, but we'll keep it for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximator_output.shape: torch.Size([1, 64, 192])\n"
     ]
    }
   ],
   "source": [
    "from upt.models.approximator import Approximator\n",
    "\n",
    "approximator = Approximator(\n",
    "    # tell the approximator the dimension of the input (perc_dim or enc_dim of encoder)\n",
    "    input_dim=192,\n",
    "    # as in ViT-T\n",
    "    dim=192,\n",
    "    num_heads=3,\n",
    "    # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "    depth=4,\n",
    ")\n",
    "\n",
    "approximator_output = approximator(encoded_pointcloud)\n",
    "print(f\"approximator_output.shape: {approximator_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The decoder now only uses some transformers, then averages all tokens and classifies the image with a simple linear\n",
    "head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction.shape: torch.Size([1, 10])\n",
      "decoder predicted class: tensor([3])\n"
     ]
    }
   ],
   "source": [
    "from upt.models.decoder_classifier import DecoderClassifier\n",
    "\n",
    "decoder = DecoderClassifier(\n",
    "    # tell the decoder the dimension of the input (dim of approximator)\n",
    "    input_dim=192,\n",
    "    # CIFAR10 has 10 classes\n",
    "    num_classes=10,\n",
    "    # as in ViT-T\n",
    "    dim=192,\n",
    "    num_heads=3,\n",
    "    # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "    depth=4,\n",
    ")\n",
    "prediction = decoder(approximator_output)\n",
    "print(f\"prediction.shape: {prediction.shape}\")\n",
    "print(f\"decoder predicted class: {prediction.argmax(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train a model\n",
    "Now we can put it all together and train an image classifier on a \"point cloud of pixels\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "parameters: 5.7M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss: ????? train_accuracy: ????% test_accuracy: ????%:   0%|                                                                                                                     | 0/1950 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 135\u001B[0m\n\u001B[1;32m    132\u001B[0m     param_group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m lrs[update]\n\u001B[1;32m    134\u001B[0m \u001B[38;5;66;03m# forward pass\u001B[39;00m\n\u001B[0;32m--> 135\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_feat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_feat\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_pos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_pos\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m    \u001B[49m\u001B[43msupernode_idxs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msupernode_idxs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_idx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    140\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m y \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget_class\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    142\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(y_hat, y)\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/upt-minimal/upt/models/upt_sparseimage_classifier.py:13\u001B[0m, in \u001B[0;36mUPTSparseImageClassifier.forward\u001B[0;34m(self, input_feat, input_pos, supernode_idxs, batch_idx)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_feat, input_pos, supernode_idxs, batch_idx):\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;66;03m# encode data\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m     latent \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_feat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_feat\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_pos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupernode_idxs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msupernode_idxs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;66;03m# propagate forward\u001B[39;00m\n\u001B[1;32m     21\u001B[0m     latent \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapproximator(latent)\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/upt-minimal/upt/models/encoder_supernodes.py:107\u001B[0m, in \u001B[0;36mEncoderSupernodes.forward\u001B[0;34m(self, input_feat, input_pos, supernode_idxs, batch_idx, condition)\u001B[0m\n\u001B[1;32m    104\u001B[0m     cond_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcond\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m condition\n\u001B[1;32m    106\u001B[0m \u001B[38;5;66;03m# supernode pooling\u001B[39;00m\n\u001B[0;32m--> 107\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msupernode_pooling\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_feat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_feat\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_pos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[43m    \u001B[49m\u001B[43msupernode_idxs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msupernode_idxs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;66;03m# project to encoder dimension\u001B[39;00m\n\u001B[1;32m    115\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menc_proj(x)\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/upt-minimal/upt/modules/supernode_pooling.py:63\u001B[0m, in \u001B[0;36mSupernodePooling.forward\u001B[0;34m(self, input_feat, input_pos, supernode_idxs, batch_idx)\u001B[0m\n\u001B[1;32m     60\u001B[0m input_edges \u001B[38;5;241m=\u001B[39m input_edges[:, is_supernode_edge]\n\u001B[1;32m     62\u001B[0m \u001B[38;5;66;03m# embed mesh\u001B[39;00m\n\u001B[0;32m---> 63\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_feat\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_embed(input_pos)\n\u001B[1;32m     65\u001B[0m \u001B[38;5;66;03m# create message input\u001B[39;00m\n\u001B[1;32m     66\u001B[0m dst_idx, src_idx \u001B[38;5;241m=\u001B[39m input_edges\u001B[38;5;241m.\u001B[39munbind()\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/kappamodules/layers/linear_projection.py:47\u001B[0m, in \u001B[0;36mLinearProjection.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/system/apps/userenv/alkin/v3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from upt.models.approximator import Approximator\n",
    "from upt.models.decoder_classifier import DecoderClassifier\n",
    "from upt.models.encoder_supernodes import EncoderSupernodes\n",
    "from upt.models.upt_sparseimage_classifier import UPTSparseImageClassifier\n",
    "from upt.datasets.sparse_cifar10_classifier_dataset import SparseCIFAR10ClassifierDataset\n",
    "from upt.collators.sparseimage_classifier_collator import SparseImageClassifierCollator\n",
    "\n",
    "# initialize dataset\n",
    "transform = ToTensor()\n",
    "train_dataset = SparseCIFAR10ClassifierDataset(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    # use half of the inputs for training (32x32 pixels = 1024)\n",
    "    num_inputs=512,\n",
    ")\n",
    "test_dataset = SparseCIFAR10ClassifierDataset(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    # use all inputs for evaluation (32x32 pixels = 1024)\n",
    "    num_inputs=1024,\n",
    ")\n",
    "\n",
    "# hyperparameters\n",
    "dim = 192  # ~6M parameter model\n",
    "num_heads = 3\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "# initialize model\n",
    "model = UPTSparseImageClassifier(\n",
    "    encoder=EncoderSupernodes(\n",
    "        # CIFAR has 3 channels (RGB)\n",
    "        input_dim=3,\n",
    "        # CIFAR is an image dataset -> 2D\n",
    "        ndim=2,\n",
    "        # there are 32x32 pixels so positions are in [0, 31], to have roughly the same input as a ViT\n",
    "        # with patch_size=4, we'll use radius slighly larger than 4\n",
    "        radius=5,\n",
    "        # if we split a 32x32 image into 8x8 gridpoints, each point would cover 4x4 pixels, i.e. 16 pixels (=nodes)\n",
    "        # since we sample supernodes randomly and use a larger radius, it can happen that more than 16 nodes\n",
    "        # are in the radius of a supernode, so we'll use at maximum 32 connections to each supernode\n",
    "        max_degree=32,\n",
    "        # dimension for the supernode pooling -> use same as ViT-T latent dim\n",
    "        gnn_dim=dim,\n",
    "        # ViT-T latent dimension\n",
    "        enc_dim=dim,\n",
    "        enc_num_heads=num_heads,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        enc_depth=4,\n",
    "        # the perceiver is optional, it changes the size of the latent space to NUM_LATENT_TOKENS tokens\n",
    "        # perc_dim=dim,\n",
    "        # perc_num_heads=num_heads,\n",
    "        # num_latent_tokens=32,\n",
    "    ),\n",
    "    approximator=Approximator(\n",
    "        # tell the approximator the dimension of the input (perc_dim or enc_dim of encoder)\n",
    "        input_dim=dim,\n",
    "        # as in ViT-T\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        depth=4,\n",
    "    ),\n",
    "    decoder=DecoderClassifier(\n",
    "        # tell the decoder the dimension of the input (dim of approximator)\n",
    "        input_dim=dim,\n",
    "        # CIFAR10 has 10 classes\n",
    "        num_classes=10,\n",
    "        # as in ViT-T\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        depth=4,\n",
    "    ),\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "\n",
    "# setup dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=SparseImageClassifierCollator(num_supernodes=64, deterministic=False),\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=SparseImageClassifierCollator(num_supernodes=64, deterministic=True),\n",
    ")\n",
    "\n",
    "# initialize optimizer and learning rate schedule (linear warmup for first 10% -> linear decay)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)\n",
    "total_updates = len(train_dataloader) * epochs\n",
    "warmup_updates = int(total_updates * 0.1)\n",
    "lrs = torch.concat(\n",
    "    [\n",
    "        # linear warmup\n",
    "        torch.linspace(0, optim.defaults[\"lr\"], warmup_updates),\n",
    "        # linear decay\n",
    "        torch.linspace(optim.defaults[\"lr\"], 0, total_updates - warmup_updates),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# train model\n",
    "update = 0\n",
    "pbar = tqdm(total=total_updates)\n",
    "pbar.update(0)\n",
    "pbar.set_description(\"train_loss: ????? train_accuracy: ????% test_accuracy: ????%\")\n",
    "test_accuracy = 0.0\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "loss = None\n",
    "train_accuracy = None\n",
    "for _ in range(epochs):\n",
    "    # train for an epoch\n",
    "    for batch in train_dataloader:\n",
    "        # schedule learning rate\n",
    "        for param_group in optim.param_groups:\n",
    "            param_group[\"lr\"] = lrs[update]\n",
    "\n",
    "        # forward pass\n",
    "        y_hat = model(\n",
    "            input_feat=batch[\"input_feat\"].to(device),\n",
    "            input_pos=batch[\"input_pos\"].to(device),\n",
    "            supernode_idxs=batch[\"supernode_idxs\"].to(device),\n",
    "            batch_idx=batch[\"batch_idx\"].to(device),\n",
    "        )\n",
    "        y = batch[\"target_class\"].to(device)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update step\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # status update\n",
    "        train_accuracy = (y_hat.argmax(dim=1) == y).sum() / y.numel()\n",
    "        update += 1\n",
    "        pbar.update()\n",
    "        pbar.set_description(\n",
    "            f\"train_loss: {loss.item():.4f} \"\n",
    "            f\"train_accuracy: {train_accuracy * 100:4.1f}% \"\n",
    "            f\"test_accuracy: {test_accuracy * 100:4.1f}%\"\n",
    "        )\n",
    "        train_losses.append(loss.item())\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # evaluate\n",
    "    num_correct = 0\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(\n",
    "                input_feat=batch[\"input_feat\"].to(device),\n",
    "                input_pos=batch[\"input_pos\"].to(device),\n",
    "                supernode_idxs=batch[\"supernode_idxs\"].to(device),\n",
    "                batch_idx=batch[\"batch_idx\"].to(device),\n",
    "            )\n",
    "        y = batch[\"target_class\"].to(device)\n",
    "        num_correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "    test_accuracy = num_correct / len(test_dataset)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    pbar.set_description(\n",
    "        f\"train_loss: {loss.item():.4f} \"\n",
    "        f\"train_accuracy: {train_accuracy * 100:4.1f}% \"\n",
    "        f\"test_accuracy: {test_accuracy * 100:4.1f}%\"\n",
    "    )\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lets plot the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "axes[0].plot(range(len(train_losses)), train_losses)\n",
    "axes[0].set_xlabel(\"Updates\")\n",
    "axes[0].set_ylabel(\"Train Loss\")\n",
    "axes[0].set_title(\"Train Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[1].plot(range(len(train_accuracies)), train_accuracies)\n",
    "axes[1].set_xlabel(\"Updates\")\n",
    "axes[1].set_ylabel(\"Train Accuracy\")\n",
    "axes[1].set_title(\"Train Accuracy\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "axes[2].plot(range(len(test_accuracies)), test_accuracies, marker=\"o\")\n",
    "axes[2].set_xlabel(\"Epochs\")\n",
    "axes[2].set_ylabel(\"Test Accuracy\")\n",
    "axes[2].set_title(\"Test Accuracy\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can also classify pointclouds with UPT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}